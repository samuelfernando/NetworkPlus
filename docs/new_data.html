<!DOCTYPE html>
<!-- Template by quackit.com -->
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>Testing on new data</title>
		<link rel="stylesheet" type="text/css" href="style.css">
		<script src="w3data.js"></script>		
			
	
	</head>
	
	<body>
		<div id="wrapper">
		
			<main>
				<div id="content">
					<div class="innertube">
						<h1>Testing on new data</h1>
<p> If you followed all the steps for <a href="installing_asr.html">installing the ASR</a> then you should have successfully tested the ASR on the WSJ test set. If you want to now run on different data, then you will need to take various steps to prepare the data in the correct format for Kaldi. You will probably also need to create a new language model that is appropriate for the test set. For serious efforts, you should read the <a href="http://kaldi-asr.org/doc/">Kaldi documentation</a>. But here I will give a few tips on how to get started.<p>

<h1>Data preparation</h1>

<p>In Network Plus you will have recorded audio of speech in various settings. You will need to do various additional preparation in order to get the data in a suitable format for processing by Kaldi. There is a section on <a href="http://kaldi-asr.org/doc/data_prep.html">data preparation</a> in the Kaldi documentation, but it can be a little difficult for beginners to understand. So I'll give here some practical tips on how to get going.</p>

<h2>Segmentation and transcription</h2>

<p>First, you will need to <b>segment</b> the audio. This means selecting those parts of the audio files that actually contain speech, and then dividing those into individual <b>utterances</b>. In order to measure the performance of the ASR on this data, you will need to get <b>gold standard</b> data, i.e. utterances which have been segmented and <b>transcribed</b> with an accurate textual representation of the utterance. However, you could use a tool such as the Google Speech API to help you to get a first pass, and then use human transcribers to adjust and correct this transcription, which may be quicker than doing it from scratch.</p>

<p>To do the segmentation and transcription I would recommend using <a href="https://www.ldc.upenn.edu/language-resources/tools/xtrans">Xtrans</a>, a software tool which is available for Windows (in theory it is also available for Linux, but I couldn't get it working after many hours of trying. However on Linux I was able to run the Windows version using Wine). This is a simple tool which allows transcribers to quickly segment and transcribe audio files.</p>

<p>Practically, I would suggest hiring students to do the transcription. I can recommend people I have used in the past. The university does recommend a transcription company, but they are very expensive, and the people I have used can do the same job for a fraction of the price.</p>



<h2>Kaldi formatting</h2>

<p>Assuming you have the audio data segmented into individual utterances, and transcribed correctly, the next step will be getting this into the correct format for Kaldi to process (either as training or test data).</p>

<p>There is detailed information on the <a href="http://kaldi-asr.org/doc/data_prep.html">data preparation</a> page in the Kaldi documentation, but basically you need to create four text files which define the data: <code>wav.scp</code> which points to the audio recordings, <code>text</code> which contains the transcriptions, <code>utt2spk</code> which contains speaker information, and <code>segments</code> which define the audio segments containing the utterances by start and end times. There are other files required, but these can be automatically generated by scripts in Kaldi.</p>

<p>For this step, you will probably want to write scripts to convert the data from the previous step into the Kaldi format. This is relatively straightforward if you used the Xtrans software. The resulting TDF files will have the segment information (start and end times of each utterance) and the text transcriptions, so can be processed to generate the <code>segments</code> and <code>text</code> files, and the <code>wav.scp</code> and <code>utt2spk</code> files can also be created easily.</p>

<h2>Language models</h2>

The other requirement is a language model (LM). In principle you could use the same LM as for the WSJ data. However this will probably give quite poor results for the test set since the utterances from Network Plus are unlikely to resemble text from the WSJ. 

Creating good language models is a difficult task and forms an entire research area in itself. However some scripts are provided in this setup which will allow reasonable quality trigram models to be created given some source text. Another option might be to create a grammar, but that is beyond the scope of this work.

To create an LM in the correct format for Kaldi requires the following steps:

<ol>

<li>A pronunciation dictionary/lexicon. This contains the total set of words that can be recognised, and for each word stores at least one phonetic sequence that represents that word. For the WSJ set I used the <code>local/make_beep_dict.sh</code> script to generate the pronunciation lexicon. This uses British English pronunciation dictionary <a href="http://svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html">Beep</a> as the basis for the lexicon, and also guesses pronunciations for OOV (out of vocabulary) words using the Sequitur tool. This script requires one input file:  a text file containing the source text from which you want to generate the language model. This file is specified near the top of the script. The output is a set of files (by default stored in <code>data/local/dict</code>) that include a list of phonemes (e.g. <code>nonsilence_phones.txt</code>, and the pronunciaton lexicon itself <code>lexicon.txt</code>). 
</li>

<li>Next you want to train the language model on the text file. This can be done by adapting the <code>local/my_train_lms.sh</code> script. This takes two inputs. The first is the text file containing the source text for the language model (probably the same file you used for the previous step), along with the pronunciation dictionary generated from the previous step. These two files are specified near the top of the script. The output is the trigram language model (by default stored in <code>data/local/lm/3gram-mincount/lm_unpruned.gz</code>.</li>

<li>Finally there are a few steps required to create language model in the script <code>local/my_create_test_lang.sh</code>. This converts the ARPA format language model from the previous step into the required FST format for Kaldi. </li>

<li>There is one more step required before this graph can be used for decoding. This is to make a decoding graph which combines the language model and the acoustic model into a single efficient FST which can be used for rapid decoding. This is done using the <code>utils/mkgraph.sh</code> script. For example if the new LM created from the previous steps is located in <code>data/netplus_lang</code>, then this command should be run:

<pre>utils/mkgraph.sh data/netplus_lang exp/tri4b exp/tri4b/netplus_graph</pre>

to combine the LM with the acoustic model in <code>exp/tri4b</code> to generate the decoding graph in <code>exp/tri4b/netplus_graph</code>.




</ol>







</ul>



			</div>
				</div>
			</main>
			
			<nav id="nav">
				<div w3-include-html="links.html" class="innertube">
				</div>
			</nav>
	<script>
w3IncludeHTML();
</script>
		</div>
	</body>
</html>